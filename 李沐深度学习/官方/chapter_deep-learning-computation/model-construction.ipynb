{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d628b46",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 层和块\n",
    "\n",
    "我们先回顾一下多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9895e279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:00.244437Z",
     "iopub.status.busy": "2023-08-18T06:57:00.243813Z",
     "iopub.status.idle": "2023-08-18T06:57:01.320999Z",
     "shell.execute_reply": "2023-08-18T06:57:01.320186Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1445,  0.0104, -0.0149, -0.1505,  0.2127,  0.1162,  0.2087,  0.2139,\n",
       "         -0.0640,  0.1559],\n",
       "        [-0.1402,  0.0562,  0.0159, -0.3024,  0.1329,  0.0990,  0.1825, -0.0074,\n",
       "         -0.1688,  0.0150]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5e7a1",
   "metadata": {},
   "source": [
    "在这个例子中，我们通过实例化nn.Sequential来构建我们的模型， 层的执行顺序是作为参数传递的。 简而言之，nn.Sequential定义了一种特殊的Module， 即在PyTorch中表示一个块的类， 它维护了一个由Module组成的有序列表。 注意，两个全连接层都是Linear类的实例， Linear类本身就是Module的子类。 另外，到目前为止，我们一直在通过net(X)调用我们的模型来获得模型的输出。 这实际上是net.__call__(X)的简写。 这个前向传播函数非常简单： 它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389483a7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "`nn.Sequential`定义了一种特殊的`Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adf2a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 自定义块:\n",
    "将输入数据作为其前向传播函数的参数。\n",
    "\n",
    "通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。\n",
    "\n",
    "计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "\n",
    "存储和访问前向传播计算所需的参数。\n",
    "\n",
    "根据需要初始化模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876df867",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.325541Z",
     "iopub.status.busy": "2023-08-18T06:57:01.324828Z",
     "iopub.status.idle": "2023-08-18T06:57:01.330411Z",
     "shell.execute_reply": "2023-08-18T06:57:01.329591Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5feff25",
   "metadata": {},
   "source": [
    "torch.nn.ReLU 是一个模块类，通常用于构建序列模型（即 nn.Sequential 模块）或者在定义自己的 nn.Module 子类时将其作为一个层包含在内。\n",
    "\n",
    "torch.nn.functional.relu 是一个函数，通常在定义自己的网络模块时，在 forward 方法中直接调用来应用激活函数。\n",
    "\n",
    "总结一下，nn.ReLU是一个层，需要在构建模型的时候就定义好，而F.relu是一个函数，只能在前向传播的函数中被调用，它更加灵活，因为它不需要事先定义模型。这两者都是内置ReLU激活函数的选择，而使用哪一种取决于个人的代码风格和特定的用例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63bddd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a34ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.334346Z",
     "iopub.status.busy": "2023-08-18T06:57:01.333603Z",
     "iopub.status.idle": "2023-08-18T06:57:01.340473Z",
     "shell.execute_reply": "2023-08-18T06:57:01.339676Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1635, -0.2636, -0.0760, -0.2374,  0.1898,  0.0729, -0.0105,  0.1855,\n",
       "          0.0559, -0.0855],\n",
       "        [ 0.1519, -0.1852,  0.0123, -0.1386,  0.0088,  0.0129,  0.0009,  0.1650,\n",
       "         -0.0139, -0.0306]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a9ee2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 顺序块\n",
    "现在我们可以更仔细地看看Sequential类是如何工作的， 回想一下Sequential的设计是为了把其他模块串起来。 为了构建我们自己的简化的MySequential， 我们只需要定义两个关键函数：\n",
    "\n",
    "一种将块逐个追加到列表中的函数；\n",
    "\n",
    "一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。\n",
    "\n",
    "下面的MySequential类提供了与默认Sequential类相同的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9672de9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.353302Z",
     "iopub.status.busy": "2023-08-18T06:57:01.352727Z",
     "iopub.status.idle": "2023-08-18T06:57:01.360268Z",
     "shell.execute_reply": "2023-08-18T06:57:01.359462Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1425,  0.1552, -0.0698,  0.0215,  0.4182,  0.4846, -0.0090, -0.2805,\n",
       "          0.0133, -0.1630],\n",
       "        [ 0.1597,  0.0818, -0.1490,  0.1249,  0.3177,  0.3237,  0.1127, -0.2987,\n",
       "          0.0641,  0.0255]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            # str(idx)通过字典的键索引\n",
    "            # _名字变量是内部使用的变量\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # .values()获得字典的values\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce57d60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "在前向传播函数中执行代码\n",
    "\n",
    " 然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项， 我们称之为常数参数（constant parameter）。 例如，我们需要一个计算函数f(x, w)=c*wTx 的层，其中x是输入,w是参数，T进行转置,c是某个在优化过程中没有更新的指定常量。 因此我们实现了一个FixedHiddenMLP类，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00ebc567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.373508Z",
     "iopub.status.busy": "2023-08-18T06:57:01.372789Z",
     "iopub.status.idle": "2023-08-18T06:57:01.380049Z",
     "shell.execute_reply": "2023-08-18T06:57:01.379025Z"
    },
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3631, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # torch.mm()对两个矩阵进行矩阵乘法操作\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a6b49",
   "metadata": {},
   "source": [
    "在这个FixedHiddenMLP模型中，我们实现了一个隐藏层， 其权重（self.rand_weight）在实例化时被随机初始化，之后为常量。 这个权重不是一个模型参数，因此它永远不会被反向传播更新。 然后，神经网络将这个固定层的输出通过一个全连接层。\n",
    "\n",
    "注意，在返回输出之前，模型做了一些不寻常的事情： 它运行了一个while循环，在L1\n",
    "范数大于1\n",
    "的条件下， 将输出向量除以2\n",
    "，直到它满足条件为止。 最后，模型返回了X中所有项的和。 注意，此操作可能不会常用于在任何实际任务中， 我们只展示如何将任意代码集成到神经网络计算的流程中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d1e5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 混合搭配各种组合块的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ca3b399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.384091Z",
     "iopub.status.busy": "2023-08-18T06:57:01.383236Z",
     "iopub.status.idle": "2023-08-18T06:57:01.394649Z",
     "shell.execute_reply": "2023-08-18T06:57:01.393535Z"
    },
    "origin_pos": 43,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0850, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "required_libs": [],
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
