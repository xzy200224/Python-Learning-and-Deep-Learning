{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23850d90",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 参数管理\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "访问参数，用于调试、诊断和可视化；\n",
    "\n",
    "参数初始化；\n",
    "\n",
    "在不同模型组件间共享参数。\n",
    "\n",
    "\n",
    "我们首先看一下具有单隐藏层的多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7ef7a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:09.649068Z",
     "iopub.status.busy": "2023-08-18T07:01:09.648305Z",
     "iopub.status.idle": "2023-08-18T07:01:10.928992Z",
     "shell.execute_reply": "2023-08-18T07:01:10.927959Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2067],\n",
       "        [0.3609]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaff55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2fff9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.933865Z",
     "iopub.status.busy": "2023-08-18T07:01:10.933267Z",
     "iopub.status.idle": "2023-08-18T07:01:10.939922Z",
     "shell.execute_reply": "2023-08-18T07:01:10.938931Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0261,  0.3525, -0.2899, -0.2765, -0.0821,  0.0569, -0.1954, -0.2978]])), ('bias', tensor([0.2537]))])\n"
     ]
    }
   ],
   "source": [
    "# net[2]指的是访问net所代表的模型中的第三个层\n",
    "# .state_dict()是一个函数，它返回该层的状态字典（state dictionary），其中包含了层中所有可学习参数（如权重和偏差）的当前值。\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e174dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "目标参数\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。 要对参数执行任何操作，首先我们需要访问底层的数值。 有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。 下面的代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0682fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.945104Z",
     "iopub.status.busy": "2023-08-18T07:01:10.944250Z",
     "iopub.status.idle": "2023-08-18T07:01:10.951764Z",
     "shell.execute_reply": "2023-08-18T07:01:10.950790Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.2537], requires_grad=True)\n",
      "tensor([0.2537])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db7c05",
   "metadata": {},
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。 这就是我们需要显式参数值的原因。 除了值之外，我们还可以访问每个参数的梯度。 在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cf4d55b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.956378Z",
     "iopub.status.busy": "2023-08-18T07:01:10.955542Z",
     "iopub.status.idle": "2023-08-18T07:01:10.961810Z",
     "shell.execute_reply": "2023-08-18T07:01:10.960767Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b54ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "一次性访问所有参数\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。 当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂， 因为我们需要递归整个树来提取每个子块的参数。 下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916939ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.966725Z",
     "iopub.status.busy": "2023-08-18T07:01:10.965969Z",
     "iopub.status.idle": "2023-08-18T07:01:10.972600Z",
     "shell.execute_reply": "2023-08-18T07:01:10.971655Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# *操作符有时被称为“解包”操作符。当与函数调用一起使用时，它会将列表、元组或其他可迭代对象中的元素解包（unpack）作为单独的参数传递给函数。\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce65fd",
   "metadata": {},
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116207ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.977269Z",
     "iopub.status.busy": "2023-08-18T07:01:10.976623Z",
     "iopub.status.idle": "2023-08-18T07:01:10.983222Z",
     "shell.execute_reply": "2023-08-18T07:01:10.982309Z"
    },
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2537])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707279d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "从嵌套块收集参数\n",
    "\n",
    "将多个块相互嵌套，参数命名约定是如何工作的。 我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712e31fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.988088Z",
     "iopub.status.busy": "2023-08-18T07:01:10.987352Z",
     "iopub.status.idle": "2023-08-18T07:01:10.998245Z",
     "shell.execute_reply": "2023-08-18T07:01:10.997197Z"
    },
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1256],\n",
       "        [0.1256]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "    # add_module 是 nn.Module 类的一个方法，用于向模型中添加子模块\n",
    "    # 每个子模块都被分配了一个唯一的名称\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a2644",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "设计了网络后，我们看看它是如何工作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d7717d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.002889Z",
     "iopub.status.busy": "2023-08-18T07:01:11.002264Z",
     "iopub.status.idle": "2023-08-18T07:01:11.007643Z",
     "shell.execute_reply": "2023-08-18T07:01:11.006464Z"
    },
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6c13a",
   "metadata": {},
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。 下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "939ba4d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.012522Z",
     "iopub.status.busy": "2023-08-18T07:01:11.011839Z",
     "iopub.status.idle": "2023-08-18T07:01:11.018508Z",
     "shell.execute_reply": "2023-08-18T07:01:11.017590Z"
    },
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2656,  0.1920, -0.4882,  0.2681, -0.3721,  0.4722, -0.1861,  0.0240])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b45fbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 内置初始化\n",
    "\n",
    "让我们首先调用内置的初始化器。 下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f00d5e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.023955Z",
     "iopub.status.busy": "2023-08-18T07:01:11.023046Z",
     "iopub.status.idle": "2023-08-18T07:01:11.033287Z",
     "shell.execute_reply": "2023-08-18T07:01:11.032096Z"
    },
    "origin_pos": 47,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0081,  0.0099, -0.0274,  0.0096]), tensor(0.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb917f2f",
   "metadata": {},
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49ee306c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.038321Z",
     "iopub.status.busy": "2023-08-18T07:01:11.037607Z",
     "iopub.status.idle": "2023-08-18T07:01:11.049009Z",
     "shell.execute_reply": "2023-08-18T07:01:11.047793Z"
    },
    "origin_pos": 52,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)  # 初始化张量为常数值\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)    # 递归地将函数应用于模型内的每个子模块\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478059aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "对某些块应用不同的初始化方法。用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。\n",
    "Xavier初始化方法是一种常用的权重初始化方法，旨在解决神经网络训练过程中梯度消失或爆炸的问题，通过根据输入和输出的连接数来合理地初始化权重，从而使得每层神经元的输出方差保持一致，避免信号在传播过程中因为权重过大或过小而消失或爆炸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a90ffaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.054335Z",
     "iopub.status.busy": "2023-08-18T07:01:11.053550Z",
     "iopub.status.idle": "2023-08-18T07:01:11.063215Z",
     "shell.execute_reply": "2023-08-18T07:01:11.062244Z"
    },
    "origin_pos": 57,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4711,  0.6265, -0.3328, -0.3693])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70ae16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9166f6e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.068164Z",
     "iopub.status.busy": "2023-08-18T07:01:11.067460Z",
     "iopub.status.idle": "2023-08-18T07:01:11.079228Z",
     "shell.execute_reply": "2023-08-18T07:01:11.078069Z"
    },
    "origin_pos": 66,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000,  8.3256, -0.0000],\n",
       "        [-0.0000, -6.3787,  6.4699,  0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        # .named_parameters() 是 PyTorch 中用于获取神经网络模型中所有参数的方法。它会返回一个迭代器，该迭代器包含每个参数的名称及其对应的参数值。\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        # nn.init.uniform_ 是 PyTorch 中用于对张量进行均匀分布初始化的函数。它可以将输入张量初始化为指定范围内的均匀分布随机值。\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        # 大于等于 5 的元素保留，而小于 5 的元素置为 0\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b9af1f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.084158Z",
     "iopub.status.busy": "2023-08-18T07:01:11.083416Z",
     "iopub.status.idle": "2023-08-18T07:01:11.092672Z",
     "shell.execute_reply": "2023-08-18T07:01:11.091537Z"
    },
    "origin_pos": 71,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000,  9.3256,  1.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031168e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 参数绑定\n",
    "\n",
    "有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n",
    "\n",
    "参数绑定的目的可能包括：\n",
    "1.  参数减少：如果网络中有重复的结构时，通过共享参数可以减少模型的大小。\n",
    "2.  正则化：参数共享可以防止过拟合，特别是在小数据集上。\n",
    "3.  特定任务的模式学习：在有些情况下，我们可能希望不同的功能部分学习到相同的模式。\n",
    "\n",
    "当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69660fa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.097767Z",
     "iopub.status.busy": "2023-08-18T07:01:11.096948Z",
     "iopub.status.idle": "2023-08-18T07:01:11.108904Z",
     "shell.execute_reply": "2023-08-18T07:01:11.107763Z"
    },
    "origin_pos": 77,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "required_libs": [],
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
