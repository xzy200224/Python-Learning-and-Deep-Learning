# 机器学习与深度学习

## 基本流程

数据预处理：处理缺失值、噪声值、错误数据、重复数据等；数据转换；数据归一化。

数据增强：

模型选择：交叉验证（6：2：2）；k折交叉验证；留一法。

评估：

对误差分析再修改：

## 如何选择激活函数

1. Sigmoid函数（Logistic函数）：
   - 范围在0到1之间，将输入映射到概率的区间。
   - 函数平滑，但存在梯度饱和问题，导致梯度消失。
   - 在输出层用于二分类问题。
2. Tanh函数（双曲正切函数）：
   - 范围在-1到1之间，对输入进行标准化。
   - 和Sigmoid函数类似，但范围更广，具有更好的非线性表示能力。
   - 可用于隐藏层，特别是在RNN中。
3. ReLU函数（线性整流函数）：
   - 对于正数，输出等于输入；对于负数，输出为0。
   - 计算速度快，不存在梯度饱和问题（最经常用）。
   - 但存在“神经元死亡”问题，即某些神经元可能永远不会被激活。
4. Leaky ReLU函数：
   - 在负数部分引入一个小的斜率，以解决ReLU的“神经元死亡”问题。
   - 适用于大规模的深度学习模型。
5. ELU函数（指数线性单元）：
   - 在负数部分引入一个指数函数，以解决ReLU的问题。
   - 对于正数，输出等于输入；对于负数，输出接近于0。
   - 具有平滑性，但计算代价较高。
6. Softmax函数：
   - 用于多分类问题，在输出层将多个神经元的输出转化为概率分布。
   - 将输入值归一化为0到1之间的概率。

## 判断过拟合和欠拟合

计算偏差和方差。

偏差可以理解为模型的错误率，即模型对于某种事物的预测结果与实际情况相差多少。如果模型存在较大的偏差，那么其预测结果可能与实际情况相差很远，这意味着模型无法很好地进行拟合，可能需要更复杂的模型或更多的数据来解决。例如，一个线性回归模型可能会存在较大的偏差，因为它只能拟合线性关系。

方差则是模型在不同样本上的预测结果波动程度的度量。如果模型的方差很大，说明模型对于训练数据过度拟合，导致在新数据上的表现不佳。这时候我们需要采取一些措施来减少模型的方差，例如增加正则化项、增加训练数据量等。

通常情况下，我们希望模型同时具备较小的偏差和方差，这意味着模型能够很好地适应训练数据，同时在新数据上表现也很好。在实际模型训练过程中，我们需要通过交叉验证等方法来评估模型的偏差和方差，并采取相应的措施来优化模型的表现。

## 提高准确率（Accuracy）的方法

(1).使用更复杂的模型：可以尝试增加模型的复杂度，例如增加更多的隐藏层或增加隐藏单元的数量。这样可以提供更多的参数来捕捉数据中的复杂关系。

(2).调整参数：包括学习率、批次大小和训练轮次等。可以尝试不同的超参数组合，找到最优的设置。可以尝试使用学习率调度器来动态地调整学习率。

(3).数据增强：通过在训练集中应用变换（例如旋转、平移和缩放），可以增加数据的多样性，有助于模型更好地泛化到新样本。

(4).正则化：可以尝试使用正则化技术（如L1或L2正则化）来限制模型的复杂度，以减少过拟合。

(5).扩大训练集：如果可能的话，尝试获取更多的训练数据，这样模型将有更多样本进行学习。

(6).尝试不同的优化器：除了随机梯度下降（SGD）之外，还可以尝试其他优化器，如Adam、RMSprop等。

(7).模型集成：可以尝试使用集成学习方法，将多个模型的预测结果进行组合，以提高准确性。

原文链接：https://blog.csdn.net/m0_66197927/article/details/133890252

## 损失Loss为Nan或者无穷大（INF）原因

### 1.原因汇总

- 脏数据：训练数据(包括label)中有无异常值(nan, inf等);
- 除0问题。这里实际上有两种可能，一种是被除数的值是无穷大，即 Nan，另一种就是0作为了除数（分母可以加一个eps=1e-8）。之前产生的 Nan 或者0，有可能会被传递下去，造成后面都是 Nan。请先检查一下神经网络中有可能会有除法的地方，例 softmax 层，再认真的检查一下数据。可以尝试加一些日志，把神经网络的中间结果输出出来，看看哪一步开始出现 Nan 。
- 可能0或者负数作为自然对数，或者 网络中有无开根号(torch.sqrt), 保证根号下>=0
- 初始参数值过大：也有可能出现 Nan 问题。输入和输出的值，最好也做一下归一化。
- 学习率设置过大：初始学习率过大，也有可能造成这个问题。如果在迭代的100轮以内，出现NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。可以不断降低学习率直至不出现NaN为止，一般来说低于现有学习率1-10倍即可。如果为了排除是不是学习率的原因，可以直接把学习率设置为0，然后观察loss是否出现Nan，如果还是出现就不是学习率的原因。需要注意的是，即使使用 adam 之类的自适应学习率算法进行训练，也有可能遇到学习率过大问题，而这类算法，一般也有一个学习率的超参，可以把这个参数改的小一些。
- 梯度过大，造成更新后的值为 Nan 。如果当前的网络是类似于RNN的循环神经网络的话，在序列比较长的时候，很容易出现梯度爆炸的问题，进而导致出现NaN，一个有效的方式是增加“gradient clipping”（梯度截断来解决）：对梯度做梯度裁剪，限制最大梯度，
- 需要计算loss的数组越界（尤其是自定义了一个新的网络，可能出现这种情况）
- 在某些涉及指数计算，可能最后算得值为 INF（无穷）（比如不做其他处理的softmax中分子分母需要计算exp（x），值过大，最后可能为INF/INF，得到NaN，此时你要确认你使用的softmax中在计算exp（x）做了相关处理（比如减去最大值等等）

###	2.解决方法

输入数据有误（脏数据）：定位错误数据,然后删掉这部分数据。

学习率过高 --> 梯度爆炸进 --> Nan：1、降低初始学习率，并且设置合适的学习速率和学习率衰减，至少降低一个数量级；2、梯度裁剪，设置gradient clipping，用于限制过大的 diff；3、数据量纲不一致，也会导致梯度爆炸，数据归一化方法（减均值，除方差，或者加入normalization，例如BN、L2 norm等）；5、注意每个batch前梯度要清零，optimizer.zero_grad()；4、如果模型中有多个loss层，就需要找到梯度爆炸的层，然后降低该层的loss weight

损失函数有误：**原因：**损失函数的计算可能导致NaN的出现，尤其是在我们自己设计损失函数的时候，如交叉熵损失函数的计算可能出现log(0)，**可能是初始化的问题，也可能是数据的问题（**输入没有归一化的值**），**所以就会出现loss为Nan的情况现象: 观测训练产生的loss时一开始并不能看到异常，loss也在逐步的下降,但是突然出现Nan
解决方法：

1、损失函数应该考虑到是否可以正常地backward。

2、其次对输入的Tensor是否进行了类型转化，保证计算中保持同一类型。

3、最后考虑在除数中加入微小的常数保证计算稳定性。

4、尝试重现该错误,在loss layer中加入一些输出以进行调试。找到可能出现的错误的地方，增加一个bias

原文链接：https://blog.csdn.net/ytusdc/article/details/122321907